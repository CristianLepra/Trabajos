{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![](https://7612750.fs1.hubspotusercontent-na1.net/hubfs/7612750/logo%20Senpai_2022%20(1)-1.png)\n",
        "# ****Proyecto final - Bootcamp Data Science****\n",
        "\n"
      ],
      "metadata": {
        "id": "G9f_jI2lgLLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grupo 2\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Nombre    | Apellido\n",
        "--------  |----------\n",
        "Matilde   | Lanza\n",
        "Santiago  | Díaz\n",
        "Cristian  | Lepra\n",
        "Joaquín   | Domínguez"
      ],
      "metadata": {
        "id": "CxVQpl8Ig6Ij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Herramientas utilizadas\n",
        "\n",
        "---\n",
        "*   Kaggle\n",
        "*   PowerPoint\n",
        "*   Neptune.ai\n"
      ],
      "metadata": {
        "id": "YTPG2jf6hHvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Desarrollo de un modelo de computer vision para la deteccion y clasificacion de leucemia linfocitica aguda de tipo B** **texto en negrita**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZWfCqm0VhcoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Marco teórico\n"
      ],
      "metadata": {
        "id": "T9h58b50h4C9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La leucemia es un grupo heterogéneo de enfermedades malignas de la médula osea que afectan la produccion de celulas sanguineas de líneas linfoides o mieloides. La leucemia linfocitica aguda (LLA) se da por una acumulacion de mutaciones en celulas progenitoras de la linea linfoide que llevan a una proliferacion de celulas linfoides inmaduras que se liberan al torrente sanguineo. Esta enfermedad provoca una sintomatologia muy diversa que avanza con rapidez.\n",
        "\n",
        "Dentro de la LLA, se encuentra la de tipo B que afecta los linfocitos B, y la de tipo T que afecta a los linfocitos T.\n",
        "\n",
        "En la LLA de tipo B se sabe que la maduración de las células leucémicas es un factor importante en el resultado de la terapia. Asimismo, el pronóstico del paciente empeora con el avance del desarrollo de las células B malignas (pre-B temprana, pre-B, pro- B) Boyett et al., (1989).\n",
        "\n",
        "El diagnóstico definitivo de la leucemia linfoblástica aguda requiere pruebas invasivas, costosas que requieren mucho tiempo. Actualmente el análisis de imágenes de frotis de sangre periférica juega un rol vital en el diagnóstico de la LLA pero puede llevar a errores de diagnóstico debido a la naturaleza inespecífica de la sintomatologia de la enfermedad.\n",
        "\n",
        "Tambien es frecuente la confusion del diagnostico de la enfermedad con casos benignos en donde se encuentran hematogonias en el frotis. Las hematogonias son precursores normales de linfocitos B que pueden ser detectados en bajan proporcion en sangre periferica en pacientes con varias condiciones clinicas. Estas celulas presentan caracteristicas mnorfologicas similares con los linfoblastos presentes de la leucemia linfoblástica B Boyett et al., (1989).\n"
      ],
      "metadata": {
        "id": "hAs61E2Ijjx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivo\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6EYjU7tEj16q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desarrollar un modelo predictivo basado en CNN para distinguir casos de Leucemia linfoblástica aguda de tipo B,  de casos negativos sospechosos, a partir de imágenes de frotis de sangre periférica."
      ],
      "metadata": {
        "id": "M3dzjwlKj3l7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalacion e importacion de librerias\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XvKgv2OjkB_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install neptune #Instalación biblioteca Neptune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKLsgThAkHIJ",
        "outputId": "6c7d8600-6f5f-40ae-bdc0-4ef582a35f1f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neptune\n",
            "  Downloading neptune-1.8.5-py3-none-any.whl (479 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.7/479.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython>=2.0.8 (from neptune)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune) (9.4.0)\n",
            "Requirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from neptune) (2.3.0)\n",
            "Collecting boto3>=1.28.0 (from neptune)\n",
            "  Downloading boto3-1.33.1-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bravado<12.0.0,>=11.0.0 (from neptune)\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (8.1.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune) (0.18.3)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from neptune) (23.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neptune) (1.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from neptune) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.31.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.16.0)\n",
            "Collecting swagger-spec-validator>=2.7.4 (from neptune)\n",
            "  Downloading swagger_spec_validator-3.0.3-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.0.7)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.6.4)\n",
            "Collecting botocore<1.34.0,>=1.33.1 (from boto3>=1.28.0->neptune)\n",
            "  Downloading botocore-1.33.1-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.28.0->neptune)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.9.0,>=0.8.0 (from boto3>=1.28.0->neptune)\n",
            "  Downloading s3transfer-0.8.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bravado-core>=5.16.1 (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading bravado_core-6.1.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0.1)\n",
            "Collecting simplejson (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting monotonic (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (4.5.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython>=2.0.8->neptune)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (2023.7.22)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune) (4.19.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (1.23.5)\n",
            "Collecting jsonref (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.13.0)\n",
            "Collecting fqdn (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting isoduration (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting rfc3339-validator (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting rfc3987 (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Collecting uri-template (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.13)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading types_python_dateutil-2.8.19.14-py3-none-any.whl (9.4 kB)\n",
            "Installing collected packages: types-python-dateutil, rfc3987, monotonic, uri-template, smmap, simplejson, rfc3339-validator, jsonref, jsonpointer, jmespath, fqdn, gitdb, botocore, arrow, s3transfer, isoduration, GitPython, swagger-spec-validator, boto3, bravado-core, bravado, neptune\n",
            "Successfully installed GitPython-3.1.40 arrow-1.3.0 boto3-1.33.1 botocore-1.33.1 bravado-11.0.3 bravado-core-6.1.0 fqdn-1.5.1 gitdb-4.0.11 isoduration-20.11.0 jmespath-1.0.1 jsonpointer-2.4 jsonref-1.1.0 monotonic-1.6 neptune-1.8.5 rfc3339-validator-0.1.4 rfc3987-1.3.8 s3transfer-0.8.0 simplejson-3.19.2 smmap-5.0.1 swagger-spec-validator-3.0.3 types-python-dateutil-2.8.19.14 uri-template-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install neptune-tensorflow-keras #Instalación biblioteca neptune-tensorflow-keras"
      ],
      "metadata": {
        "id": "5FnyUb-dkT8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tf-explain #Instalación biblioteca tf-explain"
      ],
      "metadata": {
        "id": "PzY2sSWOkc5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Carga de librerias que vamos a usar en el notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2 as cv\n",
        "import seaborn as sns\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from random import sample\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "\n",
        "from tf_explain.core import GradCAM\n",
        "\n",
        "import neptune\n",
        "from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
        "from neptune.types import File\n",
        "\n"
      ],
      "metadata": {
        "id": "bXGKq_efkziX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clases y funciones\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Zhxnor0hmOWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Creamos el try/except siguiente ya que kaggle a veces nos da problema con la ruta para cargar el set de datos, es decir, a veces usa esta ruta : '/kaggle/input/Original' y a veces esta otra : '/kaggle/input/leukemia/Original'*"
      ],
      "metadata": {
        "id": "2rNS0oNDmYyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ruta_directorio_original1 = '/kaggle/input/Original'\n",
        "ruta_directorio_original2 = '/kaggle/input/leukemia/Original'\n",
        "\n",
        "try:\n",
        "    #Intentar acceder al primer directorio\n",
        "    archivos = os.listdir(ruta_directorio_original1)\n",
        "    ruta_directorio_original = ruta_directorio_original1\n",
        "except FileNotFoundError:\n",
        "    try:\n",
        "        #Si el primer directorio no existe, intentar acceder al segundo directorio\n",
        "        archivos = os.listdir(ruta_directorio_original2)\n",
        "        ruta_directorio_original = ruta_directorio_original2\n",
        "    except FileNotFoundError:\n",
        "        print(\"Ninguno de los directorios existe.\")\n",
        "        #Puedes agregar más lógica aquí según tus necesidades, como salir del programa o asignar un valor por defecto.\n",
        "\n",
        "#Usar la ruta correcta\n",
        "print(\"Ruta correcta:\", ruta_directorio_original)"
      ],
      "metadata": {
        "id": "Xihq3_L8mKq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42 #Establecemos la semilla en 42"
      ],
      "metadata": {
        "id": "x-1M8fS-mznk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para que los resultados sean comparables debemos usar siempre el mismo random state por eso fijamos el seed y la fijamos con la funcion siguiente"
      ],
      "metadata": {
        "id": "clnw7qSnoNI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.set_random_seed(seed) #Establecer semilla para TensorFlow"
      ],
      "metadata": {
        "id": "6AoUt-nsng_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a evaluar dos modelos un CNN diseñado por nosotros y una red pre entrenada ResNet50 con fine tuning. Con esta función entrenamos ambos modelos a la vez."
      ],
      "metadata": {
        "id": "nLWX3Bayrgl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento de dos modelos en simultaneo\n",
        "def train_models(model_cnn, model_resnet, train_generator, val_generator, callbacks):\n",
        "    # Entrenar el modelo CNN\n",
        "    history_cnn = model_cnn.fit(train_generator,\n",
        "                                epochs=50,\n",
        "                                validation_data=val_generator,\n",
        "                                callbacks=callbacks)\n",
        "\n",
        "    # Entrenar el modelo ResNet\n",
        "    history_resnet = model_resnet.fit(train_generator,\n",
        "                                      epochs=50,\n",
        "                                      validation_data=val_generator,\n",
        "                                      callbacks=callbacks)\n",
        "\n",
        "    return history_cnn, history_resnet"
      ],
      "metadata": {
        "id": "azcSubwdreqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta función gráfica la historia de entrenamiento de un modelo, incluyendo la evolución de la pérdida y el recall. La función recibe un objeto history y un título como argumentos y, para cada uno de ellos, crea un gráfico de pérdida y recall."
      ],
      "metadata": {
        "id": "EEuTVua_sA5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluacion de modelos en validacion\n",
        "def plot_model_history(history, title):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Gráfica de Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss', color='cyan')\n",
        "    plt.title('Loss - ' + title)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Gráfica de Recall\n",
        "    plt.subplot(1, 2, 2)\n",
        "\n",
        "    # Buscar la métrica de recall en el historial\n",
        "    recall_metric = None\n",
        "    for metric in history.history.keys():\n",
        "        if 'recall' in metric:\n",
        "            recall_metric = metric\n",
        "            break\n",
        "\n",
        "    if recall_metric:\n",
        "        plt.plot(history.history[recall_metric], label='Train Recall', color='red')\n",
        "        val_recall_metric = 'val_' + recall_metric\n",
        "        if val_recall_metric in history.history:\n",
        "            plt.plot(history.history[val_recall_metric], label='Val Recall', color='orange')\n",
        "\n",
        "        plt.title('Recall - ' + title)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Recall')\n",
        "        plt.legend()\n",
        "    else:\n",
        "        print(\"No se encontró la métrica de recall en el historial.\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LmRAcof7r8-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Carga de datos\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HHUE0dw1ogiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos un dataset de Kaggle que posee imágenes de microscopio, contiene dos carpetas una con las imágenes originales y otra con las imágenes segmentadas, para nuestro análisis utilizamos las imágenes originales. Estas imágenes están divididas en cuatro categorías (Benign, Pro, Pre y Early). Estas categorías son las que decíamos que el modelo prediga.\n",
        "\n",
        "Dataset:\n",
        "https://www.kaggle.com/datasets/mehradaria/leukemia"
      ],
      "metadata": {
        "id": "RmYMlrHyopTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de datos\n",
        "def count_files(folder_names = []):\n",
        "    for f_name in folder_names:\n",
        "        count = len(os.listdir(f'{f_name}'))\n",
        "        print(f'Folder: {f_name} contains {count} images')"
      ],
      "metadata": {
        "id": "eT2bsF63qug1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como tenemos las imágenes en carpetas diferentes según su clasificación generamos esta función para contar cuántas imágenes hay por subcarpeta es decir por categoría."
      ],
      "metadata": {
        "id": "avOpQLWosi96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(ruta_directorio_original) # Si no corre el path usar este : '/kaggle/input/leukemia/Original'\n",
        "print(os.listdir())\n",
        "\n",
        "count_files(['Benign', 'Early', 'Pre', 'Pro'])"
      ],
      "metadata": {
        "id": "2gbWOIUmsY_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Montaje de Neptune\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "quFI6YdKsQrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Init Neptune\n",
        "run = neptune.init_run(\n",
        "        project=\"joacodominguez/proyecto-final\",\n",
        "        api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI3M2Y4OGJkNC00ZjlmLTQ5MmUtYTg5YS1mMGEzMjEzZmE3Y2QifQ==\", # your credentials\n",
        ")"
      ],
      "metadata": {
        "id": "alWxXudxtGx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_version = neptune.init_model_version(\n",
        "    model=\"PROY-PROY\",\n",
        "    project=\"joacodominguez/proyecto-final\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI3M2Y4OGJkNC00ZjlmLTQ5MmUtYTg5YS1mMGEzMjEzZmE3Y2QifQ==\", # your credentials\n",
        ")"
      ],
      "metadata": {
        "id": "caQW87L0tK86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploracion de datos"
      ],
      "metadata": {
        "id": "xN3Y1LsBtnnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función anterior recorre una lista de subcarpetas, cuenta las dimensiones únicas de las imágenes en cada subcarpeta y luego imprime el resultado."
      ],
      "metadata": {
        "id": "pnScpB0_ttgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subcarpetas = ['Benign', 'Early', 'Pre', 'Pro']  # Asumo que esta es la lista de subcarpetas\n",
        "\n",
        "unique_dimensions = set()\n",
        "\n",
        "for sub in subcarpetas:\n",
        "    folder_path = os.path.join(ruta_directorio_original, sub)\n",
        "\n",
        "    for file in os.listdir(folder_path):\n",
        "        image_path = os.path.join(folder_path, file)\n",
        "        with Image.open(image_path) as img:\n",
        "            unique_dimensions.add(img.size)\n",
        "\n",
        "if len(unique_dimensions) == 1:\n",
        "    print(f\"\\nTodas las imágenes tienen las mismas dimensiones: {unique_dimensions.pop()}\")\n",
        "else:\n",
        "    print(f\"\\nSe encontraron {len(unique_dimensions)} dimensiones únicas de imágenes: {unique_dimensions}\")"
      ],
      "metadata": {
        "id": "ejbO05NYtofI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función anterior realiza varias operaciones para mostrar una cuadrícula de 30 imágenes seleccionadas al azar de las subcarpetas."
      ],
      "metadata": {
        "id": "LD5Nn9e4uGjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa una lista para almacenar nombres de archivos de todas las subcarpetas\n",
        "nombres_archivos = []\n",
        "\n",
        "# Itera sobre las subcarpetas y obtén los nombres de archivos\n",
        "for subcarpeta in os.listdir(ruta_directorio_original):\n",
        "    ruta_subcarpeta = os.path.join(ruta_directorio_original, subcarpeta)\n",
        "    if os.path.isdir(ruta_subcarpeta):\n",
        "        nombres_archivos.extend(os.listdir(ruta_subcarpeta))\n",
        "\n",
        "# Asegúrate de que haya al menos 30 imágenes disponibles\n",
        "if len(nombres_archivos) >= 30:\n",
        "    # Selecciona al azar 30 imágenes de la lista\n",
        "    imagenes_ejemplo = sample(nombres_archivos, 30)\n",
        "\n",
        "# Configura el diseño de la figura\n",
        "    filas, columnas = 5, 6\n",
        "    figura, ejes = plt.subplots(filas, columnas, figsize=(15, 12))\n",
        "    figura.subplots_adjust(hspace=0.5)\n",
        "\n",
        "    # Itera sobre las imágenes de ejemplo y muéstralas en la cuadrícula\n",
        "    for i, nombre_archivo in enumerate(imagenes_ejemplo):\n",
        "        # Itera sobre las subcarpetas para encontrar la imagen\n",
        "        for subcarpeta in os.listdir(ruta_directorio_original):\n",
        "            ruta_subcarpeta = os.path.join(ruta_directorio_original, subcarpeta)\n",
        "            ruta_imagen = os.path.join(ruta_subcarpeta, nombre_archivo)\n",
        "\n",
        "            if os.path.isfile(ruta_imagen):\n",
        "                imagen = cv.imread(ruta_imagen)\n",
        "                imagen = cv.cvtColor(imagen, cv.COLOR_BGR2RGB)  # Convertir de BGR a RGB para mostrar con matplotlib\n",
        "                ejes[i // columnas, i % columnas].imshow(imagen)\n",
        "                ejes[i // columnas, i % columnas].axis('off')\n",
        "                ejes[i // columnas, i % columnas].set_title(subcarpeta)\n",
        "                break  # Rompe el bucle una vez que se encuentra la imagen\n",
        "\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No hay suficientes imágenes en las subcarpetas para mostrar 30 ejemplos.\")"
      ],
      "metadata": {
        "id": "culkmwwLuFih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODELADO\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DC8eNtDguWHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código está creando dos listas, data y labels, que se utilizan para almacenar la información de los archivos de imágenes y sus respectivas etiquetas."
      ],
      "metadata": {
        "id": "KN1crmywuhiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de los archivos y los labels\n",
        "data = [os.path.join(folder,file) for folder in os.listdir(ruta_directorio_original) for file in os.listdir(os.path.join(ruta_directorio_original, folder))]\n",
        "labels = [folder for folder in os.listdir(ruta_directorio_original) for file in os.listdir(os.path.join(ruta_directorio_original, folder))]"
      ],
      "metadata": {
        "id": "SmX2WKc3wFol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acá se realiza la división de los datos en conjuntos de entrenamiento, validación y prueba, y luego configura generadores de datos para el aumento de imágenes y la preparación de los datos para el entrenamiento y la evaluación del modelo."
      ],
      "metadata": {
        "id": "f2j-QfSIwuFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "data_temp,data_test, labels_temp,labels_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "data_train, data_val, labels_train, labels_val = train_test_split(data_temp, labels_temp, test_size=0.1, random_state=42)\n",
        "\n",
        "# data generators para aumentacion en train con rescale\n",
        "datagen = ImageDataGenerator(rescale=1./255,  # rescale pixel a [0,1]\n",
        "                             rotation_range=40,\n",
        "                             width_shift_range=0.2,\n",
        "                             height_shift_range=0.2,\n",
        "                             shear_range=0.2,\n",
        "                             zoom_range=0.2,\n",
        "                             horizontal_flip=True,\n",
        "                             fill_mode='nearest')\n",
        "\n",
        "train_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=pd.DataFrame({'filename': data_train, 'label': labels_train}),\n",
        "    directory=ruta_directorio_original,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(224,224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=pd.DataFrame({'filename': data_val, 'label': labels_val}),\n",
        "    directory=ruta_directorio_original,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(224,224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# saco la aumentacion para el test\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=pd.DataFrame({'filename': data_test, 'label': labels_test}),\n",
        "    directory=ruta_directorio_original,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(224,224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "id": "r-HBKI9eucgA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}