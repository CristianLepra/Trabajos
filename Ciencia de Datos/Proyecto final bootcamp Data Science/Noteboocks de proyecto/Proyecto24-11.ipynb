{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Proyecto final grupo 2**","metadata":{}},{"cell_type":"markdown","source":"# Clases y funciones\n\nEn el siguiente apartado se recomienda definir las clases/funciones/funciones lambdas aplicables al desarrollo. Esto permitirá tener organizado la declaración de las mismas así como la actualización.\n\n>Note: Todas las clases/funciones deben poseer la documentación correspondiente de los parámetros que recibe, argumentos, y salida de las mismas","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"seed = 42\nruta_directorio_original = '/kaggle/input/Original'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Librerias y pip install**","metadata":{}},{"cell_type":"code","source":"pip install neptune\npip install tf-explain --quiet","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2 as cv\nfrom tensorflow import keras\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom PIL import Image\nfrom random import sample\nimport albumentations as A\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.losses import SparseCategoricalCrossentropy\nfrom keras.optimizers import Adam\nfrom keras.utils import plot_model\nfrom tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy, SparseCategoricalAccuracy\nimport neptune\nfrom neptune.integrations.tensorflow_keras import NeptuneCallback\nfrom neptune.types import File\nfrom tf_explain.core import GradCAM\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nfrom keras.applications.resnet import ResNet50","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Neptune**","metadata":{}},{"cell_type":"code","source":"#Init Neptune\nrun = neptune.init_run(\n        project=\"joacodominguez/proyecto-final\",\n        api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI3M2Y4OGJkNC00ZjlmLTQ5MmUtYTg5YS1mMGEzMjEzZmE3Y2QifQ==\", # your credentials\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_version = neptune.init_model_version(\n    model=\"PROY-PROY\",\n    project=\"joacodominguez/proyecto-final\",\n    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI3M2Y4OGJkNC00ZjlmLTQ5MmUtYTg5YS1mMGEzMjEzZmE3Y2QifQ==\", # your credentials\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Carga de datos","metadata":{}},{"cell_type":"code","source":"os.chdir('/kaggle/input/Original')\nprint(os.listdir())\n\ndef count_files(folder_names = []):\n    for f_name in folder_names:\n        count = len(os.listdir(f'{f_name}'))\n        print(f'Folder: {f_name} contains {count} images')\n        \ncount_files(['Benign', 'Early', 'Pre', 'Pro'])subcarpeta = ['Benign', 'Early', 'Pre', 'Pro']  # Asumo que esta es tu lista de subcarpetas\n\nunique_dimensions = set()\n\nfor sub in subcarpeta:\n    folder_path = os.path.join(ruta_directorio_original, sub)\n\n    for file in os.listdir(folder_path):\n        image_path = os.path.join(folder_path, file)\n        with Image.open(image_path) as img:\n            unique_dimensions.add(img.size)\n\nif len(unique_dimensions) == 1:\n    print(f\"\\nTodas las imágenes tienen las mismas dimensiones: {unique_dimensions.pop()}\")\nelse:\n    print(f\"\\nSe encontraron {len(unique_dimensions)} dimensiones únicas de imágenes: {unique_dimensions}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inicializa una lista para almacenar nombres de archivos de todas las subcarpetas\nnombres_archivos = []\n\n# Itera sobre las subcarpetas y obtén los nombres de archivos\nfor subcarpeta in os.listdir(ruta_directorio_original):\n    ruta_subcarpeta = os.path.join(ruta_directorio_original, subcarpeta)\n    if os.path.isdir(ruta_subcarpeta):\n        nombres_archivos.extend(os.listdir(ruta_subcarpeta))\n\n# Asegúrate de que haya al menos 30 imágenes disponibles\nif len(nombres_archivos) >= 30:\n    # Selecciona al azar 30 imágenes de la lista\n    imagenes_ejemplo = sample(nombres_archivos, 30)\n\n# Configura el diseño de la figura\n    filas, columnas = 5, 6\n    figura, ejes = plt.subplots(filas, columnas, figsize=(15, 12))\n    figura.subplots_adjust(hspace=0.5)\n\n    # Itera sobre las imágenes de ejemplo y muéstralas en la cuadrícula\n    for i, nombre_archivo in enumerate(imagenes_ejemplo):\n        # Itera sobre las subcarpetas para encontrar la imagen\n        for subcarpeta in os.listdir(ruta_directorio_original):\n            ruta_subcarpeta = os.path.join(ruta_directorio_original, subcarpeta)\n            ruta_imagen = os.path.join(ruta_subcarpeta, nombre_archivo)\n            \n            if os.path.isfile(ruta_imagen):\n                imagen = cv.imread(ruta_imagen)\n                imagen = cv.cvtColor(imagen, cv.COLOR_BGR2RGB)  # Convertir de BGR a RGB para mostrar con matplotlib\n                ejes[i // columnas, i % columnas].imshow(imagen)\n                ejes[i // columnas, i % columnas].axis('off')\n                ejes[i // columnas, i % columnas].set_title(subcarpeta)\n                break  # Rompe el bucle una vez que se encuentra la imagen\n\n    plt.show()\nelse:\n    print(\"No hay suficientes imágenes en las subcarpetas para mostrar 30 ejemplos.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalizacion y preprocesamiento","metadata":{}},{"cell_type":"code","source":"def preprocessor(image):\n    # Applying normalization to the image\n    image = cv.normalize(image, None, 0, 255, cv.NORM_MINMAX)    \n    \n    image = np.array(image)/255.0\n    return image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MODELADO, Train/Test/Split","metadata":{}},{"cell_type":"code","source":"all_data = []\n\n# Crear un diccionario con el nombre de la subcarpeta y su índice\nlabel_mapping = {subcarpeta: i for i, subcarpeta in enumerate(subcarpetas)}\n\n# Itera sobre cada subcarpeta\nfor subcarpeta in subcarpetas:\n    ruta_subcarpeta = os.path.join(ruta_directorio_original, subcarpeta)\n\n    # Asegúrate de que sea una subcarpeta y no un archivo\n    if os.path.isdir(ruta_subcarpeta):\n        # Itera sobre cada imagen en la subcarpeta\n        for nombre_archivo in os.listdir(ruta_subcarpeta):\n            ruta_imagen = os.path.join(ruta_subcarpeta, nombre_archivo)\n\n            # Asegúrate de que sea un archivo y no una subcarpeta\n            if os.path.isfile(ruta_imagen):\n                # Lee la imagen\n                imagen = cv.imread(ruta_imagen)\n                imagen = cv.resize(imagen, (224, 224)) / 255.0\n                \n                # Mapeo de la etiqueta a valor numérico usando el diccionario label_mapping\n                etiqueta = label_mapping[subcarpeta]\n\n                all_data.append([imagen, etiqueta])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convertir la lista a arrays de Numpy\nx = np.array(img_matrix_list)\ny = np.array([label_mapping[etiqueta] for etiqueta in etiquetas_list])\n\n# Realizar la división entre conjuntos de entrenamiento, prueba y validación\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=seed)\n\n# Dividir adicionalmente el conjunto de entrenamiento en un conjunto de validación\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)\n\nprint(x_train.shape, x_test.shape, x_val.shape, y_train.shape, y_test.shape, y_val.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data augmentation (albumentation)","metadata":{}},{"cell_type":"code","source":"def plot_multiple_img(img_matrix_list, title_list, ncols, main_title=\"\"):\n    fig, axis = plt.subplots(figsize=(15, 8), nrows=2, ncols=ncols, squeeze=False)\n    fig.suptitle(main_title, fontsize=18)\n    fig.subplots_adjust(wspace=0.3)\n    fig.subplots_adjust(hspace=0.3)\n\n    for i, (img, title) in enumerate(zip(img_matrix_list, title_list)):\n        axis[i // ncols][i % ncols].imshow(img)\n        axis[i // ncols][i % ncols].set_title(title, fontsize=15)\n\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definir la lista de transformaciones de Albumentations\nalbumentation_list = [\n    A.RandomFog(p=1),\n    A.VerticalFlip(p=1),\n    A.RandomBrightness(p=1),\n    A.RandomContrast(limit=0.6, p=1),\n    A.RandomCrop(p=1, height=224, width=224),\n    A.Rotate(p=1, limit=90),\n    A.RGBShift(p=1)\n]\n\n# Obtener la lista de nombres de subcarpetas (clases)\nsubcarpetas = os.listdir(ruta_directorio_original)\n\n# Lista para almacenar las imágenes originales y augmentadas\nimg_matrix_list = []\netiquetas_list =[]\n\n# Iterar sobre cada subcarpeta\nfor subcarpeta in subcarpetas:\n    ruta_subcarpeta = os.path.join(ruta_directorio_original, subcarpeta)\n\n    # Asegurarse de que sea una subcarpeta y no un archivo\n    if os.path.isdir(ruta_subcarpeta):\n        # Iterar sobre cada imagen en la subcarpeta\n        for nombre_archivo in os.listdir(ruta_subcarpeta):\n            ruta_imagen = os.path.join(ruta_subcarpeta, nombre_archivo)\n\n            # Asegurarse de que sea un archivo y no una subcarpeta\n            if os.path.isfile(ruta_imagen):\n                # Leer la imagen original\n                chosen_image = cv.imread(ruta_imagen)\n\n                # Aplicar augmentations y almacenar las imágenes\n                for aug_type in albumentation_list:\n                    augmented_img = aug_type(image=chosen_image)['image']\n                    img_matrix_list.append(augmented_img)\n                    etiquetas_list.append(subcarpeta)\n\n                img_matrix_list.append(chosen_image)  # Agregar la imagen original\n                etiquetas_list.append(subcarpeta)\n# Títulos para las imágenes\ntitles_list = [\"Original\", \"RandomFog\", \"VerticalFlip\", \"RandomContrast\", \"RandomBrightness\", \"RandomCrop\", \"Rotate\", \"RGBShift\"]\n\n# Mostrar las imágenes en una cuadrícula\nplot_multiple_img(img_matrix_list, titles_list, ncols=4, main_title=\"Different Types of Augmentations\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Desarrollo de la Arquitectura CNN","metadata":{}},{"cell_type":"code","source":"tf.keras.utils.set_random_seed(seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define el modelo\nmodelo = Sequential()\nmodelo.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3),padding = 'same'))\nmodelo.add(MaxPooling2D((2, 2)))\nmodelo.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\nmodelo.add(MaxPooling2D((2, 2)))\nmodelo.add(Conv2D(128, (3, 3), activation='relu',padding='same'))\nmodelo.add(MaxPooling2D((2, 2)))\nmodelo.add(Flatten())\nmodelo.add(Dense(128, activation='relu'))\nmodelo.add(Dropout(0.5))  # Agrega Dropout con una tasa del 50% (ajusta según sea necesario)\nmodelo.add(Dense(4, activation='softmax')) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelo.compile(optimizer='Adam',\n              loss=SparseCategoricalCrossentropy(),\n              metrics=['sparse_categorical_accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelo.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Definiendo Callbacks","metadata":{}},{"cell_type":"code","source":"# Neptune parameters\nparameters = {\"dense_units\": 4,\n              \"activation\": \"relu\",\n              \"batch_size\": 64,\n              \"n_epochs\": 20\n              }\n\nrun[\"model/parameters\"] = parameters","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [\n    EarlyStopping(monitor='recall', min_delta=0, patience=8, mode='auto'),\n    ModelCheckpoint('best_model2.h5', monitor='val_recall', save_best_only=True),\n    NeptuneCallback(run=run)\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(modelo, to_file='/kaggle/working/cnn.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = modelo.fit(x_train, y_train,\n                    epochs=20,\n                    batch_size=64,\n                    validation_data=(x_val, y_val),\n                    callbacks=callbacks)  # Sin envolver en otra lista","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluación del modelo","metadata":{}},{"cell_type":"code","source":"fig = plt.figure()\nplt.plot(history.history['loss'], color='teal', label='loss')\nplt.plot(history.history['val_loss'], color='orange', label='val_loss')\nfig.suptitle('Loss', fontsize=20)\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure()\nplt.plot(history.history['sparse_categorical_accuracy'], color='teal', label='acc')\nplt.plot(history.history['val_sparse_categorical_accuracy'], color='orange', label='val_acc')\nfig.suptitle('Accuracy', fontsize=20)\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelo.save('/kaggle/working/cnn_model.h5')\n#model_resnet.save('/kaggle/working/resnet_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_accuracy = modelo.evaluate(x_test,y_test)\nprint(f'Test accuracy: {test_accuracy:.2f}, Test loss: {test_loss:.2f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"run[\"evaluation/accuracy\"] = test_accuracy\nrun[\"evaluation/loss\"] = test_loss\nmodel_version[\"model\"].upload(\"best_model.h5\") \"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtener predicciones en el conjunto de prueba\ny_pred = modelo.predict(x_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\n\n# Obtener la matriz de confusión\nconf_matrix = confusion_matrix(y_test, y_pred_classes)\n\n# Visualizar la matriz de confusión\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=subcarpetas, yticklabels=subcarpetas)\nplt.title('Matriz de Confusión')\nplt.xlabel('Predicciones')\nplt.ylabel('Etiquetas Verdaderas')\nplt.show()\n\n# Imprimir el reporte de clasificación\nprint(classification_report(y_test, y_pred_classes, target_names=subcarpetas))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calcular métricas\nspecificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\nfalse_positive_rate = conf_matrix[0, 1] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n\n# Visualización\nplt.figure(figsize=(8, 6))\nsns.barplot(x=['Especificidad', 'Tasa de Falsos Positivos'], y=[specificity, false_positive_rate], palette=\"viridis\")\nplt.title('Especificidad y Tasa de Falsos Positivos')\nplt.ylabel('Valor')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtén las probabilidades predichas para cada clase\ny_prob = modelo.predict(x_test)\n\n# Binariza las etiquetas (one-hot encoding)\ny_true_bin = label_binarize(y_test, classes=[0, 1, 2, 3])\n\n# Inicializa las variables para almacenar las tasas TPR y FPR para cada clase\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\n# Calcula las tasas TPR y FPR para cada clase\nfor i in range(4):  # 4 clases en tu caso\n    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Tamaño de la figura\nplt.figure(figsize=(8, 6))\n\n# Plotea las curvas ROC para cada clase\nfor i in range(4):\n    plt.plot(fpr[i], tpr[i], label=f'Clase {i} (AUC = {roc_auc[i]:.2f}')\n\n# Configuración de la figura\nplt.plot([0, 1], [0, 1], 'k--', label='Aleatorio')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Tasa de Falsos Positivos (FPR)')\nplt.ylabel('Tasa de Verdaderos Positivos (TPR)')\nplt.title('Curva ROC Multiclase')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiation of the explainer - GradCam()\nexplainer = GradCAM()  # TODO: Asegúrate de que GradCAM esté correctamente definido en tu código\n\n# Obtener la etiqueta (clase) de la imagen\nindex = 5   # Cambiar '1' por cualquier número entre 0 y len(y_test) - 1\nlabel = int(y_test[index])\n\n# Expandir las dimensiones de la imagen para que coincida con las expectativas de la red\nimg_to_explain = np.expand_dims(x_test[index], 0)\n\n# Llamada al método explain()\noutput = explainer.explain(validation_data=[img_to_explain, None],  # Cambié 'validation_data' por [img_to_explain, None]\n                           model=modelo, \n                           class_index=label)\n\n# Visualizar la imagen\nfig,ax = plt.subplots(1,2)\nax[0].imshow(x_test[index].squeeze())\nax[1].imshow(output)\nax[0].set_title(label)\nax[1].set_title(label)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiation of the explainer - GradCam()\nexplainer = GradCAM()  # Asegúrate de que GradCAM esté correctamente definido en tu código\n\n# Obtener 10 índices aleatorios para seleccionar 10 imágenes\nsample_indices = np.random.choice(len(y_test), size=10, replace=False)\n\n# Iterar sobre los índices de la muestra\nfor index in sample_indices:\n    # Obtener la etiqueta (clase) de la imagen\n    label = int(y_test[index])\n\n    # Expandir las dimensiones de la imagen para que coincida con las expectativas de la red\n    img_to_explain = np.expand_dims(x_test[index], 0)\n\n    # Llamada al método explain()\n    output = explainer.explain(validation_data=[img_to_explain, None],  # Cambié 'validation_data' por [img_to_explain, None]\n                               model=modelo, \n                               class_index=label)\n\n    # Visualizar la imagen y su explicación GradCAM\n    fig, ax = plt.subplots(1, 2)\n    ax[0].imshow(x_test[index].squeeze(), cmap='gray')  # Asumo que las imágenes son en escala de grises\n    ax[1].imshow(output, cmap='jet')  # Puedes ajustar el mapa de colores según tus preferencias\n    ax[0].set_title(f'Clase: {label}')\n    ax[1].set_title(f'GradCAM para Clase: {label}')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crear modelo ResNet50\nmodel_resnet = Sequential()\nmodel_resnet.add(ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3)))\nmodel_resnet.add(MaxPooling2D())\nmodel_resnet.add(Flatten())\nmodel_resnet.add(Dropout(0.3))\nmodel_resnet.add(Dense(150, activation='relu'))\nmodel_resnet.add(Dense(4, activation='softmax'))\nmodel_resnet.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n\n\n# Guardar el modelo como imagen\nplot_model(model_resnet, to_file='/kaggle/working/resnet.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_resnet.summary()","metadata":{},"execution_count":null,"outputs":[]}]}